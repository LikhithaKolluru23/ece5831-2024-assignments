{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of classes, training steps, accuracy graph and trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_hat, y): # y_hat is the predicted value, y is the actual value\n",
    "    \"\"\" Implementation of the mean squared error function \"\"\"\n",
    "    return np.sum((y_hat - y)**2)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025000000000000022"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y      = np.array([1,   2,   3,    4])\n",
    "y_hat1 = np.array([1.2, 1.9, 2.9,  4.2]) \n",
    "mean_squared_error(y_hat1, y) # mean squared error for the given values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0250000000000004"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2 = np.array([2.2, 0.9, 2.9,  5.2]) \n",
    "# mean squared error for the given values\n",
    "mean_squared_error(y_hat2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of cross entropy error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat, y): # cross entropy error function\n",
    "    \"\"\" Implementation of cross entropy error function \"\"\"\n",
    "    return -np.sum(y*np.log(y_hat + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 0, 0, 0]) # actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat1 = np.array([0.1, 0.7, 0.1, 0.1, 0]) # predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3566748010815999"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat1, y) # cross entropy error for the given values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat2 = np.array([0.7, 0.05, 0.05, 0.2, 0]) # predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9957302735559908"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat2, y) # cross entropy error for the given values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Mnist data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle: dataset/mnist.pkl already exists.\n",
      "Loading...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from mnist_data import MnistData # importing the MnistData class from mnist_data.py\n",
    "my_mnist = MnistData() # creating an object of the MnistData class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = my_mnist.load() # loading the mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_images.shape[0] # number of training images\n",
    "batch_size = 32 # batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33085 35761 18165 54676 23916  9785 23198  7278 44993 37739 41420  9202\n",
      " 56778 20345 58509 15125 14741 30361 42689 22450  8151 18370 38507 55965\n",
      " 59564 48778 15223  2365 44933 33124 35491 19381]\n"
     ]
    }
   ],
   "source": [
    "batch_mask = np.random.choice(train_size, batch_size) # randomly selecting 32 images from the training data\n",
    "print(batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mini-batch training.\n",
    "def cross_entropy_error(y_hat, y): # cross entropy error function\n",
    "    \"\"\" Implementaion of cross entropy error function for mini-batch training\"\"\"\n",
    "    batch_size = 1 if y_hat.ndim == 1 else y_hat.shape[0]\n",
    "    return -np.sum(y*np.log(y_hat + 1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_batch = np.array([ [0.2, 0.2, 0.3, 0.1, 0.2], [0.1, 0.1, 0.1, 0.1, 0.6]])\n",
    "y_batch =     np.array([ [0,   0,   1,   0,    0],   [0,   0,   0,   0,   1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573989640459981"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat_batch, y_batch) # cross entropy error for the given values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e+48"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.1/10e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x): # numerical differentiation\n",
    "    \"\"\" Implementation of numerical differentiation for a given function f and a point x\"\"\"\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x): # function to be differentiated\n",
    "    return x**2 + 0.1*x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.8) # numerical differentiation of the function at x = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more reasonable approximation for the derivative\n",
    "def numerical_diff(f, x): \n",
    "    \"\"\" Implementation of numerical differentiation \"\"\"\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6999999999994797"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.8) # numerical differentiation of the function at x = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49999999999994493"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func, 0.2) # numerical differentiation of the function at x = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x): # function to be differentiated\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def func_tmp1(x0): # function to be differentiated\n",
    "    return x0**2 + 4.0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func_tmp1, 3.0) # numerical differentiation of the function at x = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_tmp2(x1):\n",
    "    return 3.0**2 + x1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(func_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivatives when x0 = 3, x1 = 4\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "def func_tmp2(x1):\n",
    "    return 3**2 + x1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_diff(f, x): # numerical differentiation\n",
    "    \"\"\" Implementation of numerical differentiation \"\"\"\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient(f, x): # numerical gradient\n",
    "    \"\"\"Implementation of numerical gradient for a function f at point x\"\"\"\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) \n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) \n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val \n",
    "        \n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_gradient(func2, np.array([3.0, 4.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.1, step_num = 100): # gradient descent function for optimization\n",
    "    \"\"\" implement the gradient descent algorithm for optimization\"\"\"\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = _numerical_gradient(f, x)\n",
    "        x -= lr*grad  # x = x - lr*grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.65680105e-06, 2.02028609e-06])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([2800.0, 1000.0])\n",
    "# func2 = x0**2 + x1**2\n",
    "gradient_descent(func2, init_x, step_num=10000, lr=0.001) # optimization of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of SimpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    \"\"\" \n",
    "    A simple neural network with a single layer, implementing forward propagation,\n",
    "    softmax activation, and cross-entropy loss. This network is used for multi-class\n",
    "    classification problems.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the SimpleNet with random weights for the input layer.\n",
    "        \"\"\"\n",
    "        self.w = np.random.randn(2, 3)\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        \"\"\"  Computes the softmax activation function.\n",
    "\n",
    "        Softmax converts raw output scores (logits) into probability distributions \n",
    "        over classes by normalizing them.\"\"\"\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        \"\"\" Computes the cross-entropy error between the predicted output `y` and the true labels `t`.\n",
    "\n",
    "        The cross-entropy loss is commonly used for classification problems. It calculates\n",
    "        the difference between the predicted probabilities and the true class labels.\"\"\"\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def numerical_gradient(self, f, x):\n",
    "        \"\"\"  Computes the numerical gradient of a function `f` with respect to `x` using finite differences. \"\"\"\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Performs a forward pass through the network and computes the predicted output. \"\"\"\n",
    "        return np.dot(x, self.w)\n",
    "    \n",
    "\n",
    "    def loss(self, x, y):\n",
    "        \"\"\" Computes the loss (cross-entropy error) for a given input `x` and true label `y`.\n",
    "\n",
    "        The loss is computed by first passing the input through the network to get the\n",
    "        predicted output and then calculating the cross-entropy loss.\"\"\"\n",
    "        z = self.predict(x)\n",
    "        y_hat = self.softmax(z)\n",
    "        loss = self.cross_entroy_error(y_hat, y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13566702 -0.95093142  0.56273777]\n",
      " [-0.84326872  0.32122329 -1.48741569]]\n"
     ]
    }
   ],
   "source": [
    "# testing SimpleNet\n",
    "net = SimpleNet() # creating an object of the SimpleNet class\n",
    "print(net.w) # weights of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06525415 -0.60461957  0.11130746]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.7, 0.19]) # input to the network\n",
    "p = net.predict(x) # prediction of the network\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) # index of the maximum value in the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5604574334774888"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 1, 0]) # true label\n",
    "net.loss(x, y) # loss of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8445306482681669"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 0, 1]) # true label\n",
    "net.loss(x, y) # loss for the given input and true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(w): # loss function\n",
    "    return net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25214079  0.14702787 -0.39916866]\n",
      " [ 0.06843821  0.03990757 -0.10834578]]\n"
     ]
    }
   ],
   "source": [
    "dw = net.numerical_gradient(loss_function, net.w) # gradient of the loss function\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lamda\n",
    "loss_function = lambda w: net.loss(x, y) # loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25214079  0.14702787 -0.39916866]\n",
      " [ 0.06843821  0.03990757 -0.10834578]]\n"
     ]
    }
   ],
   "source": [
    "dw = net.numerical_gradient(loss_function, net.w) # gradient of the loss function\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of TwoLayerNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    \"\"\" A class to handle different types of activations for neural networks.\"\"\"\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function implementation\"\"\"\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function implementation\"\"\"\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errors:\n",
    "    \"\"\" A class to handle different types of errors for neural networks.\"\"\"\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        \"\"\"Calculates the cross-entropy error between the predicted and true labels.\"\"\"\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activations\n",
    "import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \"\"\"A simple two-layer neural network with one hidden layer, used for classification tasks.\n",
    "\n",
    "    This class implements a feedforward neural network with one hidden layer. The network \n",
    "    uses the sigmoid activation function for the hidden layer and softmax activation for \n",
    "    the output layer. It also includes methods for computing the loss (cross-entropy), \n",
    "    predicting the output, and calculating the accuracy. Gradient computation is performed \n",
    "    numerically. \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \"\"\"Initializes the two-layer neural network with random weights and biases.\"\"\"\n",
    "        self.params = {}\n",
    "\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.activations = activations.Activations()\n",
    "        self.errors = errors.Errors()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Performs a forward pass through the network and returns the predicted output. \"\"\"\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = self.activations.sigmoid(a1)\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = self.activations.softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\"Computes the loss (cross-entropy) for a given input and true label. \"\"\"\n",
    "        y_hat = self.predict(x)\n",
    "\n",
    "        return self.errors.cross_entropy_error(y_hat, y)\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        \"\"\"Calculates the accuracy of the network for a given input and true label. \"\"\"\n",
    "        y_hat = self.predict(x)\n",
    "        p = np.argmax(y_hat, axis=1)\n",
    "        y_p = np.argmax(y, axis=1)\n",
    "\n",
    "        return np.sum(p == y_p)/float(x.shape[0])\n",
    "    \n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def _numerical_gradient(self, f, x):\n",
    "        \"\"\"Computes the numerical gradient of a function `f` with respect to `x` using finite differences.\"\"\"\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "    \n",
    "\n",
    "    def numerical_gradient(self, x, y):\n",
    "        \"\"\"Computes the numerical gradient of the loss function with respect to the weights.\"\"\"\n",
    "        loss_w = lambda w: self.loss(x, y)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self._numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = self._numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = self._numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = self._numerical_gradient(loss_w, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_data import MnistData\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle: dataset/mnist.pkl already exists.\n",
      "Loading...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "my_mnist = MnistData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = my_mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=100, output_size=10) # creating an object of the TwoLayerNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[-0.00711662,  0.01334689,  0.00053283, ..., -0.00439923,\n",
       "          0.00933688, -0.00413704],\n",
       "        [ 0.00937496, -0.00334308, -0.0043983 , ..., -0.00304115,\n",
       "          0.01632827, -0.00340226],\n",
       "        [-0.00171596,  0.00688549,  0.00086869, ...,  0.01601761,\n",
       "          0.00346632,  0.00999029],\n",
       "        ...,\n",
       "        [-0.01353928, -0.00156907,  0.01366574, ...,  0.00753212,\n",
       "          0.00895732, -0.0066174 ],\n",
       "        [ 0.03274926, -0.00576418,  0.00083674, ..., -0.00400445,\n",
       "         -0.00984168, -0.00178521],\n",
       "        [ 0.01219621,  0.00717279,  0.00182846, ...,  0.002376  ,\n",
       "         -0.00851964,  0.0046679 ]]),\n",
       " 'b1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'w2': array([[ 1.60072583e-02, -1.13397344e-03, -7.22522721e-04,\n",
       "         -5.73543342e-03,  1.18399376e-04,  5.08096454e-03,\n",
       "         -9.63108947e-03, -9.13848057e-03,  3.64182654e-03,\n",
       "         -1.84276278e-03],\n",
       "        [-1.11023064e-02, -1.44144696e-02,  4.58504048e-03,\n",
       "          1.75759029e-02, -1.98875443e-03,  1.18685398e-02,\n",
       "         -1.37529281e-02,  6.63854887e-03, -2.64467086e-03,\n",
       "         -1.71057684e-02],\n",
       "        [-4.15809013e-03,  1.20466259e-03, -5.82946677e-03,\n",
       "         -6.16252742e-03,  2.17321336e-02,  1.93835127e-02,\n",
       "         -3.39207209e-03, -4.60045024e-03,  1.60651397e-02,\n",
       "          1.42145352e-02],\n",
       "        [ 1.01780265e-02,  1.12269527e-02,  5.35627529e-03,\n",
       "          9.20933099e-03, -4.44646146e-03, -7.05364743e-03,\n",
       "         -1.04496844e-02,  1.48965492e-03, -3.69234479e-03,\n",
       "          3.57089719e-03],\n",
       "        [-1.30283975e-02, -5.68261331e-03,  3.20458297e-03,\n",
       "          2.37843693e-03, -1.65068369e-02, -8.67692575e-03,\n",
       "          2.00546362e-02, -2.08721217e-03, -1.25800628e-02,\n",
       "          7.92427797e-03],\n",
       "        [-5.72040956e-03, -1.85900069e-03,  6.83322426e-03,\n",
       "          2.41806020e-03, -3.04647905e-03, -2.15550159e-03,\n",
       "          3.21867065e-04, -3.16930484e-03, -1.11073642e-02,\n",
       "          2.75074443e-03],\n",
       "        [-1.09116611e-02,  1.61280964e-02,  2.19677926e-02,\n",
       "          1.11779346e-03,  1.06892175e-02,  1.19672042e-02,\n",
       "          2.03674430e-02, -1.43343417e-02, -1.86281660e-04,\n",
       "         -1.17220898e-02],\n",
       "        [-1.92751539e-02, -4.34277192e-03, -2.30943006e-04,\n",
       "          2.57100837e-03, -1.68432575e-02, -1.34896703e-03,\n",
       "          5.25105626e-04, -5.29969046e-03,  2.04597661e-02,\n",
       "         -3.08201168e-03],\n",
       "        [-5.09513692e-03,  5.31225450e-03, -2.13904147e-03,\n",
       "          9.87934421e-03,  1.33273877e-02,  7.64477761e-03,\n",
       "          1.03463579e-02, -4.07064852e-03,  1.37588986e-02,\n",
       "          6.67260882e-03],\n",
       "        [ 8.84517370e-04, -5.04177726e-03, -1.58475080e-03,\n",
       "          8.31439289e-03, -1.60031322e-02, -1.46604925e-02,\n",
       "         -8.16865961e-03,  1.39495337e-02, -3.84745499e-03,\n",
       "         -8.39449748e-04],\n",
       "        [ 2.30066768e-03,  9.20384266e-03, -2.50809158e-03,\n",
       "         -1.25503156e-02,  1.58135825e-02,  3.51008992e-03,\n",
       "         -1.44121355e-02,  8.88928178e-03, -1.83003878e-02,\n",
       "         -2.53622277e-02],\n",
       "        [-1.17736454e-02, -1.11183442e-02, -6.72707489e-03,\n",
       "          6.27096666e-03, -9.94191485e-03,  2.93664701e-03,\n",
       "          1.19678602e-02, -1.82188957e-02,  1.36138205e-03,\n",
       "          8.64161786e-03],\n",
       "        [-4.73074306e-03,  9.04317728e-03, -9.01788618e-03,\n",
       "          7.47740976e-03,  2.30037929e-03, -6.48299669e-03,\n",
       "         -7.03069977e-03,  1.31899937e-02, -1.61729216e-03,\n",
       "         -2.38778087e-02],\n",
       "        [-7.70592704e-03,  1.05201852e-02, -2.85544086e-03,\n",
       "         -3.21159985e-03,  4.32841546e-03,  8.30525950e-04,\n",
       "         -6.04646005e-04, -5.74602729e-03, -1.08043217e-02,\n",
       "          3.14183236e-03],\n",
       "        [ 6.06578383e-03,  1.41723850e-02,  7.93559271e-03,\n",
       "         -1.00530803e-02, -6.44920697e-03,  1.69620710e-02,\n",
       "         -5.73220506e-03, -1.16270696e-03,  6.30283717e-03,\n",
       "         -6.78389234e-04],\n",
       "        [ 7.16058366e-03,  1.18127563e-02,  1.04891936e-02,\n",
       "         -1.47040471e-02,  2.96234265e-04, -8.23094668e-03,\n",
       "         -6.17451873e-03, -3.25618807e-03, -5.57990399e-03,\n",
       "         -1.67845237e-02],\n",
       "        [ 6.17595440e-03, -6.26670318e-03, -3.07061448e-03,\n",
       "          1.23789637e-02,  1.36762041e-02,  1.59508864e-02,\n",
       "          5.28754576e-04, -2.05511466e-02,  2.33602401e-02,\n",
       "          1.32579313e-02],\n",
       "        [-4.07861189e-03, -1.00290713e-02, -1.12503432e-02,\n",
       "         -7.66543450e-03,  1.66179303e-02,  4.23410306e-03,\n",
       "          9.99024721e-03,  9.06905156e-03, -1.14013623e-02,\n",
       "         -7.87599315e-03],\n",
       "        [-4.85818733e-03, -9.92451540e-03, -9.38437495e-03,\n",
       "         -2.04476369e-02, -3.52944297e-03, -3.10193015e-03,\n",
       "         -6.97085896e-03, -3.66144763e-03,  3.08397272e-03,\n",
       "         -3.21367677e-03],\n",
       "        [ 2.77285586e-04,  1.13639420e-02, -2.17792069e-03,\n",
       "         -1.00171511e-02,  1.46094835e-02,  5.55240943e-03,\n",
       "          4.84380766e-03, -3.45416709e-03, -1.86962817e-02,\n",
       "          5.88872572e-03],\n",
       "        [-2.09089687e-02,  1.87678922e-02, -1.60617949e-02,\n",
       "          4.57058245e-03,  4.83985787e-03,  6.58869354e-03,\n",
       "         -8.41854214e-03, -5.67976019e-03,  1.29898402e-02,\n",
       "         -2.32590587e-03],\n",
       "        [-3.94558574e-03, -8.36459630e-03,  4.92884084e-03,\n",
       "         -1.35135750e-02, -7.89443930e-03,  2.99261678e-03,\n",
       "         -1.38657906e-02, -1.90596016e-03, -5.26101337e-03,\n",
       "          2.65677334e-03],\n",
       "        [-2.58273426e-04, -1.31143465e-02,  1.83873238e-02,\n",
       "          4.91487358e-03, -1.08261699e-02, -1.84933296e-02,\n",
       "         -7.34128670e-03, -1.93617980e-02, -9.34665878e-03,\n",
       "         -6.48099605e-03],\n",
       "        [-1.42601242e-02,  4.91037757e-03,  1.45893911e-03,\n",
       "         -7.77156917e-03, -1.15508244e-02, -7.25354857e-03,\n",
       "          1.17298283e-02, -3.07513033e-03, -7.92998733e-03,\n",
       "         -8.07639989e-04],\n",
       "        [-4.62613174e-03,  1.66575023e-03,  5.33772551e-03,\n",
       "         -3.70465988e-03, -2.82164252e-02, -1.72833327e-02,\n",
       "         -3.26635776e-03,  3.96384663e-04,  9.60742390e-03,\n",
       "         -1.55700362e-02],\n",
       "        [-7.68358345e-04,  6.25776754e-03,  8.11924699e-03,\n",
       "          3.08746922e-03, -3.17074474e-03, -1.50400655e-03,\n",
       "          5.15708242e-04,  1.55051313e-02,  6.58444310e-03,\n",
       "          1.41155336e-02],\n",
       "        [ 1.16969356e-02, -2.36635737e-03, -8.23097413e-03,\n",
       "         -3.84686662e-03, -1.66995324e-03,  1.21349731e-02,\n",
       "          1.92835163e-03,  6.78157296e-03,  3.94660738e-04,\n",
       "          1.89677744e-02],\n",
       "        [-3.72702809e-04, -1.88823255e-03, -3.82994746e-03,\n",
       "         -8.90294566e-03, -5.46237040e-03,  1.38142067e-02,\n",
       "         -2.43674199e-03, -2.00492815e-03, -7.03992320e-03,\n",
       "          6.83940707e-03],\n",
       "        [ 8.69144565e-03,  1.81536877e-02, -2.51642833e-02,\n",
       "          3.78395717e-03,  2.27897571e-03,  2.87288703e-03,\n",
       "          4.19868739e-03, -8.28786203e-03, -8.21559313e-03,\n",
       "          3.77297402e-03],\n",
       "        [-5.27912452e-04,  2.72736184e-02, -1.71782963e-03,\n",
       "         -1.83676776e-02, -9.63597571e-03, -2.22659948e-03,\n",
       "          2.68415770e-03, -2.49904514e-05, -8.86749306e-03,\n",
       "          1.34084245e-02],\n",
       "        [ 1.11282145e-02,  4.01592393e-03, -1.11492817e-02,\n",
       "          4.14929124e-03, -2.68818307e-04, -5.80701643e-03,\n",
       "         -1.57796001e-02,  1.92806133e-02,  1.44702067e-02,\n",
       "          2.66035016e-03],\n",
       "        [-2.18364060e-03,  5.18548406e-03,  2.69390610e-03,\n",
       "          3.02267439e-03, -1.45191644e-04,  8.20743013e-03,\n",
       "          1.93675143e-03, -1.02964641e-02, -8.68149998e-04,\n",
       "         -2.36764756e-03],\n",
       "        [-4.39324737e-03, -8.28233136e-03, -1.35072292e-02,\n",
       "          6.31311729e-03, -6.70954761e-03,  2.75949873e-03,\n",
       "         -1.69290177e-03, -1.34952686e-02,  2.02602774e-03,\n",
       "          3.53581311e-03],\n",
       "        [-8.64062272e-03,  4.67913247e-04, -5.80691896e-03,\n",
       "         -2.73147547e-04,  1.03276993e-02,  7.80565168e-03,\n",
       "          1.20116159e-04, -1.19979120e-02,  9.58439216e-05,\n",
       "         -6.27408452e-03],\n",
       "        [-5.76024205e-03,  1.02352007e-04, -5.69993629e-03,\n",
       "          1.66095987e-02, -2.00763543e-02, -1.40831510e-03,\n",
       "         -1.00566886e-02,  5.27251439e-03, -4.34519309e-03,\n",
       "         -8.20920571e-03],\n",
       "        [ 1.19548714e-02, -6.01820509e-04, -6.43528872e-03,\n",
       "          1.21469075e-02,  7.11487580e-03,  9.35288686e-03,\n",
       "          8.16998136e-03,  5.12397140e-03,  2.11885743e-02,\n",
       "          3.31013398e-03],\n",
       "        [ 1.38162417e-02,  7.28039650e-03, -1.34515012e-02,\n",
       "          1.24524886e-02,  3.71971624e-03,  6.95088154e-03,\n",
       "          3.03737443e-03, -1.06744941e-02, -5.17277916e-03,\n",
       "         -6.25939962e-03],\n",
       "        [ 2.99810960e-03, -7.68483483e-03,  8.04882955e-03,\n",
       "          1.14784432e-03,  7.45527457e-03, -8.86663504e-03,\n",
       "          9.05072887e-03,  9.02146647e-03,  1.04037380e-02,\n",
       "         -2.85247856e-03],\n",
       "        [ 1.80607750e-02, -1.27142892e-02,  2.00637523e-02,\n",
       "          1.50059506e-02,  1.53171314e-03,  1.66594415e-02,\n",
       "          1.93223471e-02,  1.19176416e-04, -7.36851043e-03,\n",
       "         -2.02170232e-02],\n",
       "        [-3.85438002e-03,  9.70239561e-03, -3.45119382e-03,\n",
       "         -2.37726754e-03, -1.54603955e-03, -5.21281783e-04,\n",
       "         -2.52868746e-03,  3.50938420e-03, -1.35203175e-02,\n",
       "          3.73623069e-04],\n",
       "        [-3.24330652e-04, -1.62491543e-02, -7.47655100e-03,\n",
       "         -9.16731073e-03,  7.34614823e-03,  1.54310145e-02,\n",
       "         -1.44228225e-02,  5.19229982e-03, -1.95733874e-03,\n",
       "         -2.52683640e-02],\n",
       "        [-6.61369424e-03, -6.38230354e-03, -7.32634825e-03,\n",
       "         -7.38180478e-03, -9.21080756e-03, -8.21444693e-03,\n",
       "          2.38447107e-02, -1.84357870e-02, -1.65008825e-02,\n",
       "          1.57969546e-02],\n",
       "        [-4.93230149e-03, -3.72459625e-03, -4.77701634e-03,\n",
       "         -1.17349155e-03, -1.50504941e-02, -1.38602183e-03,\n",
       "          6.05289553e-03,  1.13767610e-03,  1.08076367e-02,\n",
       "         -1.01090442e-02],\n",
       "        [-6.13137101e-03,  1.53326926e-02, -3.23524541e-04,\n",
       "          2.73647263e-03,  7.77196963e-03,  1.21736408e-03,\n",
       "          1.13346595e-02, -8.34229379e-03, -9.79013324e-04,\n",
       "         -9.57287646e-03],\n",
       "        [-7.28075635e-03,  5.30530425e-03,  7.36068590e-03,\n",
       "         -4.57037944e-03,  8.90435138e-03, -1.93343323e-02,\n",
       "          9.73883498e-03, -9.64481282e-03, -7.08061312e-03,\n",
       "          4.86359804e-03],\n",
       "        [ 1.21064166e-03,  4.54028479e-03, -9.14886046e-03,\n",
       "         -3.30110285e-03, -6.57348012e-03,  1.72426586e-03,\n",
       "          6.16505749e-03,  1.32258813e-02,  1.04140575e-02,\n",
       "          2.97077443e-05],\n",
       "        [ 1.00588415e-02,  2.19983342e-04,  4.15188006e-03,\n",
       "          1.78807317e-02,  6.25502404e-03, -1.89801239e-02,\n",
       "          3.62144049e-03,  3.75914002e-03, -7.50172149e-03,\n",
       "          2.72328094e-02],\n",
       "        [-5.33687953e-04, -2.33933592e-03,  1.73120337e-02,\n",
       "         -1.32763369e-02, -1.70626833e-02, -1.84122147e-02,\n",
       "         -2.17832042e-03, -1.54524644e-02, -8.78354721e-03,\n",
       "          2.13226442e-04],\n",
       "        [ 2.14192418e-02,  2.03559003e-03, -7.96763967e-03,\n",
       "          2.58049135e-03,  4.84554044e-03, -2.04743064e-03,\n",
       "         -5.81047184e-03,  1.31270718e-02, -2.61270361e-04,\n",
       "          3.81421195e-03],\n",
       "        [-7.66826921e-04, -2.71509580e-03, -5.07588455e-03,\n",
       "          3.42602231e-04, -6.48474388e-03,  1.93204634e-02,\n",
       "          9.86441467e-03,  1.14078659e-03, -1.04100907e-02,\n",
       "         -1.56780144e-02],\n",
       "        [ 1.12100826e-02, -9.28689467e-03, -4.15108305e-03,\n",
       "         -2.21033627e-02, -4.08805901e-03,  2.96396892e-03,\n",
       "          1.57699463e-02,  5.96433866e-03, -1.52767771e-02,\n",
       "         -1.01079974e-03],\n",
       "        [-4.12495802e-03,  7.40024213e-03, -4.61018792e-03,\n",
       "          6.54609073e-03, -6.22175689e-03,  1.79721021e-02,\n",
       "          1.08289915e-02, -2.35752319e-03, -8.33408571e-03,\n",
       "          9.65552036e-03],\n",
       "        [ 7.73137351e-03, -2.66913751e-03,  1.11814254e-02,\n",
       "         -9.15576815e-03, -9.28879049e-03, -1.71329641e-02,\n",
       "          2.05921449e-03,  5.19836244e-03,  4.88007490e-03,\n",
       "          1.42246421e-02],\n",
       "        [ 7.85793203e-03,  6.59671162e-03, -3.50187697e-03,\n",
       "         -7.13820755e-03,  2.65738349e-03,  9.18030184e-04,\n",
       "         -1.09064994e-02,  1.25825471e-03,  4.11154087e-03,\n",
       "          4.33284196e-03],\n",
       "        [ 7.28167589e-04, -5.04017627e-03, -4.65196482e-03,\n",
       "          2.20452416e-03,  8.52199550e-03, -3.43325186e-03,\n",
       "         -7.38270101e-04, -6.02153411e-03, -1.72233488e-02,\n",
       "          5.91401829e-04],\n",
       "        [ 4.44031822e-03, -1.98324399e-02,  2.89979044e-03,\n",
       "         -1.18734629e-02,  3.95550057e-03, -4.95644571e-03,\n",
       "         -1.17689872e-02,  8.28237125e-03, -2.94107807e-03,\n",
       "         -1.04792786e-02],\n",
       "        [ 1.67457878e-02,  1.31313867e-02,  1.06958022e-03,\n",
       "         -3.03687511e-03, -1.36091443e-03, -3.76149400e-03,\n",
       "          1.21060644e-02, -8.22736683e-03,  1.21595372e-02,\n",
       "         -4.26401484e-03],\n",
       "        [-7.27250559e-03, -1.33400398e-02, -3.82988713e-03,\n",
       "          2.83096491e-03, -6.26559569e-03,  5.06548065e-03,\n",
       "         -1.90039243e-03, -2.17773758e-03,  1.10001965e-04,\n",
       "          4.77057071e-03],\n",
       "        [ 1.14677144e-02, -7.01630219e-03, -8.07956911e-04,\n",
       "          6.10895601e-03, -8.23797917e-03, -6.57148246e-03,\n",
       "          1.29691506e-02,  1.08377807e-02,  7.65812224e-03,\n",
       "         -1.20736832e-02],\n",
       "        [-1.44461670e-03, -6.67772243e-04,  3.05537018e-03,\n",
       "         -2.03932729e-02,  8.63437080e-03,  7.93687163e-04,\n",
       "          2.09658432e-03,  1.87227241e-03, -2.23150191e-03,\n",
       "         -3.09030580e-03],\n",
       "        [ 1.58054707e-02,  6.78027770e-03, -1.03680920e-02,\n",
       "          3.00511003e-04,  1.93846170e-02,  2.20611374e-02,\n",
       "          3.60629081e-03,  5.27961851e-03,  7.43370835e-03,\n",
       "         -2.74351834e-03],\n",
       "        [ 1.04079406e-02, -1.66150415e-02,  1.94804785e-02,\n",
       "         -6.20725472e-03,  5.74735596e-03, -1.01951570e-02,\n",
       "          9.49919726e-03,  6.62099932e-03, -1.48789700e-02,\n",
       "          4.41469216e-03],\n",
       "        [ 7.84493448e-03, -1.12226570e-02, -9.64457455e-03,\n",
       "         -1.86352516e-02, -2.50581856e-03, -1.88463757e-02,\n",
       "          9.14051219e-03, -5.77517941e-03,  2.03504593e-03,\n",
       "          4.61244732e-04],\n",
       "        [ 8.97437403e-03,  5.66748940e-03,  2.57270149e-03,\n",
       "         -2.72684407e-03, -5.05539589e-03, -1.25425946e-02,\n",
       "         -1.00948033e-03,  2.39377717e-03, -5.34354207e-03,\n",
       "          3.88769490e-03],\n",
       "        [-1.68314194e-02, -6.51006964e-03,  2.07173318e-02,\n",
       "          4.47033975e-03, -7.91583068e-03,  1.99407808e-03,\n",
       "          5.94636194e-03,  1.24854532e-03, -1.71708482e-03,\n",
       "         -4.15458225e-03],\n",
       "        [-4.16109099e-03, -1.68171988e-03, -2.16151200e-02,\n",
       "          1.22655623e-02,  5.31371078e-04, -7.45513937e-04,\n",
       "          1.06444543e-03,  3.65037654e-02,  1.06073859e-02,\n",
       "         -7.67416747e-03],\n",
       "        [ 1.20017197e-03, -4.58193109e-03,  1.03077210e-02,\n",
       "         -9.64112461e-03, -6.64729290e-03, -1.33758754e-02,\n",
       "          3.68273547e-03, -1.32625879e-02, -2.53363305e-03,\n",
       "         -2.80547189e-03],\n",
       "        [-1.38663805e-02,  1.50774340e-02,  6.04053836e-03,\n",
       "          1.20864036e-02,  6.50969340e-03,  4.27081236e-03,\n",
       "         -3.97754491e-03, -6.95891584e-03,  1.74615834e-03,\n",
       "         -1.63708761e-02],\n",
       "        [-1.15730125e-02,  3.13486408e-03,  9.66145548e-03,\n",
       "          1.27987842e-02,  2.27535893e-04,  1.10714489e-02,\n",
       "         -8.43355204e-03, -1.61778704e-03, -1.66843241e-02,\n",
       "         -7.92556461e-05],\n",
       "        [ 2.57498643e-03,  3.17262408e-03,  5.42209258e-03,\n",
       "         -3.27511990e-03,  1.68150764e-02, -4.51433136e-03,\n",
       "         -1.28228162e-02,  7.60952906e-03,  1.12689918e-02,\n",
       "         -7.17097614e-03],\n",
       "        [ 2.24931517e-03, -3.94026623e-03,  1.29751488e-02,\n",
       "          3.58097309e-03, -5.16341810e-04,  5.51746170e-04,\n",
       "          6.67006643e-03,  5.38205420e-03, -4.01567004e-04,\n",
       "          5.40664007e-03],\n",
       "        [ 1.61115703e-03,  3.83498036e-03, -1.19078001e-02,\n",
       "          1.77766612e-03, -9.16906816e-03, -4.12225451e-03,\n",
       "         -1.74633574e-02, -8.50315163e-03,  6.65727042e-03,\n",
       "          2.15832175e-04],\n",
       "        [-1.76569150e-02, -1.35814982e-02,  8.87240130e-03,\n",
       "         -8.02736791e-03, -7.87800949e-03, -6.56283620e-04,\n",
       "          7.11029096e-03, -1.91241365e-03,  8.59604832e-03,\n",
       "          7.52194470e-03],\n",
       "        [ 1.00469730e-03, -5.38777482e-03,  7.37751187e-03,\n",
       "         -1.20060529e-02,  6.79577494e-03, -3.44543336e-03,\n",
       "          1.21538786e-02,  8.25756477e-05, -9.61635062e-03,\n",
       "          1.29092370e-02],\n",
       "        [ 1.53832541e-03,  5.61920287e-03, -1.11123605e-02,\n",
       "          9.06730749e-03, -1.99265802e-03, -7.71827322e-03,\n",
       "          4.93654962e-03, -1.18131152e-02, -7.13574402e-03,\n",
       "          9.17259200e-03],\n",
       "        [ 1.68759988e-03,  9.28780123e-03, -7.23999157e-03,\n",
       "          4.03499318e-03,  1.14935872e-02,  1.58174452e-02,\n",
       "          1.72432931e-02,  7.81558198e-03,  5.64203452e-03,\n",
       "         -1.33226785e-02],\n",
       "        [-8.29097525e-05,  9.04794575e-03,  9.11395637e-03,\n",
       "          3.10125662e-03, -3.33910027e-03,  2.54383139e-03,\n",
       "         -2.42977387e-03, -1.18113223e-02, -1.46201682e-03,\n",
       "          5.68272753e-03],\n",
       "        [-9.10989683e-03, -5.15255236e-03,  1.21933679e-02,\n",
       "          2.33864016e-03,  1.12222815e-02, -3.93244527e-03,\n",
       "          1.40612331e-03,  2.47377443e-03, -1.49540918e-02,\n",
       "          1.38763716e-02],\n",
       "        [ 1.28944284e-02,  9.91054493e-03, -6.33821940e-03,\n",
       "         -5.62756644e-03,  2.34186873e-03, -7.90219712e-03,\n",
       "         -2.52758287e-02, -3.77911762e-03,  1.46080018e-02,\n",
       "         -1.13711073e-03],\n",
       "        [ 6.52158774e-03, -1.60817076e-02, -6.79404695e-03,\n",
       "         -9.02943694e-03, -5.61801716e-03, -4.27772236e-03,\n",
       "         -2.20151779e-03, -3.33904170e-03, -6.58646616e-03,\n",
       "          6.35225629e-03],\n",
       "        [-5.72463079e-04,  3.33919435e-03, -9.47922037e-03,\n",
       "         -2.14931376e-02,  1.58296154e-02,  2.33191454e-03,\n",
       "         -1.38624932e-02, -6.79417192e-03, -1.58777698e-03,\n",
       "         -4.33368668e-03],\n",
       "        [ 1.71363190e-03,  7.02669093e-03,  4.50408536e-03,\n",
       "          3.00341264e-03,  1.09676253e-02, -1.27301218e-02,\n",
       "         -2.53045324e-02, -4.10930890e-03,  4.89008610e-03,\n",
       "         -9.55960654e-03],\n",
       "        [ 1.32875779e-02,  6.07117330e-04, -1.67225885e-03,\n",
       "          1.58580515e-02,  2.40216586e-03,  1.10545230e-03,\n",
       "         -1.16169375e-02, -3.46672426e-03,  2.03088393e-03,\n",
       "          1.11008223e-02],\n",
       "        [-1.76660916e-03, -8.37689897e-03,  8.28983088e-03,\n",
       "          8.08099819e-03, -4.58017557e-04,  9.99929129e-03,\n",
       "          1.18478445e-02, -4.29450452e-03, -9.21298568e-03,\n",
       "         -2.10863042e-03],\n",
       "        [ 9.95578308e-03,  4.19019012e-03,  1.55257842e-02,\n",
       "         -1.70013217e-02,  4.14935228e-03, -1.60892233e-03,\n",
       "          4.17967477e-03, -1.52204164e-02,  1.09895440e-02,\n",
       "         -7.61324537e-03],\n",
       "        [-7.23401608e-03, -1.54746034e-02, -1.85160688e-02,\n",
       "         -5.64984519e-03,  4.32050242e-03, -1.42133910e-03,\n",
       "          2.05895014e-02, -1.41670694e-02, -1.62406079e-02,\n",
       "          8.16254868e-03],\n",
       "        [-4.55914407e-03,  1.21509863e-02,  6.56096999e-03,\n",
       "         -9.45207802e-03,  2.00777913e-03, -1.12518089e-02,\n",
       "          1.13513660e-03, -6.73158769e-03,  5.58504347e-03,\n",
       "          2.19529927e-02],\n",
       "        [ 2.26403642e-03,  1.18559051e-02, -1.71690139e-02,\n",
       "         -1.79757766e-04, -1.13283767e-02, -1.33936236e-02,\n",
       "         -2.69400103e-03,  1.41714892e-02,  1.14483717e-02,\n",
       "          1.55062392e-02],\n",
       "        [-5.58891575e-03, -5.64705341e-03, -2.05768625e-03,\n",
       "         -9.76974595e-03,  3.64238464e-03,  9.57283576e-03,\n",
       "         -4.65202965e-03, -1.14864393e-02,  1.60050916e-03,\n",
       "         -1.04171771e-02],\n",
       "        [ 4.22057328e-03, -5.61585417e-03, -1.37836671e-02,\n",
       "          1.66788231e-04,  2.29193985e-03,  1.75418506e-02,\n",
       "         -6.55305345e-03,  1.34145471e-02,  1.40395454e-03,\n",
       "         -5.96263074e-03],\n",
       "        [-3.56888536e-03,  5.22639437e-04, -1.84899802e-02,\n",
       "         -5.69342343e-03, -1.53705631e-03, -8.10138368e-03,\n",
       "          1.38954959e-02, -8.32290121e-04, -1.12161781e-03,\n",
       "          3.57765027e-04],\n",
       "        [ 3.75816269e-03,  1.41422106e-02, -4.89401655e-03,\n",
       "         -4.33412581e-03, -2.27275454e-03,  4.53745363e-03,\n",
       "          1.38566143e-02, -2.91974155e-03, -2.50285444e-02,\n",
       "          9.33656273e-03],\n",
       "        [ 2.63693705e-03,  3.55602845e-03, -1.91129701e-03,\n",
       "         -1.97537605e-03, -8.72253141e-03, -2.78693149e-02,\n",
       "          1.83205972e-02,  8.41915549e-04,  1.45742136e-02,\n",
       "          6.78675261e-03],\n",
       "        [-6.23552615e-03,  3.05538824e-03, -5.58919896e-03,\n",
       "         -8.75772369e-03,  4.97159430e-03, -1.26257761e-03,\n",
       "          9.68844426e-04,  9.36966215e-03, -1.05048420e-02,\n",
       "          4.46482156e-03],\n",
       "        [-2.37313747e-03, -1.22193876e-02,  1.14634954e-02,\n",
       "          5.69025760e-03,  8.16878272e-03, -1.23697378e-02,\n",
       "         -5.68201876e-03,  4.38959069e-03,  1.52646566e-02,\n",
       "          6.99340235e-03],\n",
       "        [-9.37011319e-03,  1.03319362e-02, -1.95093711e-04,\n",
       "         -6.50270437e-04,  9.33322123e-03, -2.50364693e-02,\n",
       "          1.38957117e-02, -1.41142088e-03,  1.23554911e-02,\n",
       "         -1.22966776e-03],\n",
       "        [-6.96307359e-03, -2.39654350e-02,  4.92718212e-03,\n",
       "         -1.33204687e-02,  1.01970908e-02,  6.88319575e-03,\n",
       "          8.86083730e-03,  2.28353460e-03,  6.61562894e-04,\n",
       "          1.54440637e-02],\n",
       "        [ 5.06981033e-03, -2.11331867e-02,  2.27716901e-03,\n",
       "         -3.00390109e-03, -7.46077705e-03, -1.12323865e-02,\n",
       "         -6.71935489e-04,  1.49058568e-03, -7.46467878e-03,\n",
       "         -4.13017994e-03],\n",
       "        [-6.89917660e-03,  6.03042715e-04,  1.31759085e-02,\n",
       "          1.73925771e-02, -1.58028314e-02,  1.49556613e-03,\n",
       "         -2.46567827e-03, -1.27035901e-02,  2.36315470e-03,\n",
       "          2.41041805e-02],\n",
       "        [ 6.02264931e-03,  1.23912066e-03, -9.25984177e-03,\n",
       "          3.29394403e-03,  5.70595700e-03,  3.57962784e-03,\n",
       "         -8.27497084e-04,  6.68834526e-03,  3.17161098e-04,\n",
       "          2.42911478e-04]]),\n",
       " 'b2': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.params # weights and biases of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10 #10000\n",
    "train_size = x_train[:100].shape[0]\n",
    "batch_size = 2 #100\n",
    "lr = 0.1\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "print(iter_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test_acc : 0.09871666666666666, 0.098\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations): # training the network\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    grads = network.numerical_gradient(x_batch, y_batch)\n",
    "\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        network.params[key] -= lr*grads[key]\n",
    "\n",
    "    ## this is for plotting losses over time\n",
    "    train_losses.append(network.loss(x_batch, y_batch))\n",
    "\n",
    "    if i%int(iter_per_epoch) == 0:\n",
    "        train_acc = network.accuracy(x_train, y_train)\n",
    "        train_accs.append(train_acc)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'train acc, test_acc : {train_acc}, {test_acc}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'} # plotted for 10 outputs from the above and stopped the cell as it' taking too long to run\n",
    "x = np.arange(len(train_accs))\n",
    "plt.plot(x, train_accs, label='train acc')\n",
    "plt.plot(x, test_accs, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"Implemenatation of the ReLU activation function.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of the ReLU activation function.\"\"\"\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward pass of the ReLU activation function.\"\"\"\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Implementation of the Sigmoid activation function.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        self.activations = Activations()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" forward pass of the Sigmoid activation function.\"\"\"\n",
    "        out = self.activations.sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward pass of the Sigmoid activation function.\"\"\"\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    \"\"\" Implementation of the Affine layer in a neural network.\"\"\"\n",
    "    def __init__(self, w, b):\n",
    "        \"\"\" initializes the Affine layer with weights and biases.\"\"\"\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" forward pass of the Affine layer.\"\"\"\n",
    "        # \n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.w) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward pass of the Affine layer.\"\"\"\n",
    "        dx = np.dot(dout, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  \n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    \"\"\" Implementation of the Softmax activation function with cross-entropy loss.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loss = None \n",
    "        self.y_hat = None    \n",
    "        self.y = None    \n",
    "        self.activations = Activations()\n",
    "        self.errors = Errors()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\" forward pass of the Softmax function with cross-entropy loss.\"\"\"\n",
    "        self.y = y\n",
    "        self.y_hat = self.activations.softmax(x)\n",
    "        self.loss = self.errors.cross_entropy_error(self.y_hat, self.y)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \"\"\" backward pass of the Softmax function with cross-entropy loss.\"\"\"\n",
    "        batch_size = self.y.shape[0]\n",
    "        #if self.y.size == self.y_hat.size: # one hot encoding\n",
    "        \n",
    "        dx = (self.y_hat - self.y) / batch_size\n",
    "        \n",
    "        \"\"\"\n",
    "        else:\n",
    "            dx = self.y_hat.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \"\"\"\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Layer Net with Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activations import Activations\n",
    "from errors import Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetWithBackProp:\n",
    "    \"\"\"\n",
    "    A two-layer neural network with backpropagation.\n",
    "    The network has:\n",
    "        - One hidden layer with ReLU activation.\n",
    "        - One output layer with softmax and cross-entropy loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \"\"\" Initializes the two-layer neural network with specified sizes for the input,\n",
    "        hidden, and output layers. It also initializes the weights and biases. \"\"\"\n",
    "        self.params = {}\n",
    "\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.activations = Activations()\n",
    "        self.errors = Errors()\n",
    "\n",
    "        # add layers\n",
    "        self.layers = OrderedDict()\n",
    "        self.update_layers()\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "\n",
    "    def update_layers(self):\n",
    "        \"\"\"\n",
    "        Updates the layers in the network. It includes:\n",
    "            - An affine layer (linear transformation) followed by ReLU for the hidden layer.\n",
    "            - An affine layer for the output layer.\n",
    "        \"\"\"\n",
    "        self.layers['Affine1'] = Affine(self.params['w1'], self.params['b1'])\n",
    "        self.layers['Rele1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['w2'], self.params['b2'])\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Performs a forward pass through the network and returns the predicted output. \"\"\"\n",
    "        ## new implementation for backprop\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        y = x\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        \"\"\" Computes the loss (cross-entropy) for a given input and true label. \"\"\"\n",
    "        y_hat = self.predict(x)\n",
    "\n",
    "        # return self.errors.cross_entropy_error(y_hat, y)\n",
    "        return self.last_layer.forward(y_hat, y)\n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        \"\"\" Calculates the accuracy of the network for a given input and true label. \"\"\"\n",
    "        y_hat = self.predict(x)\n",
    "        p = np.argmax(y_hat, axis=1)\n",
    "        y_p = np.argmax(y, axis=1)\n",
    "\n",
    "        return np.sum(p == y_p)/float(x.shape[0])\n",
    "    \n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        \"\"\" Computes the gradient of the loss function with respect to the weights using backpropagation. \"\"\"\n",
    "        self.loss(x, y)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self.layers['Affine1'].dw\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['w2'] = self.layers['Affine2'].dw\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "      \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_data import MnistData # importing the MnistData class from mnist_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle: dataset/mnist.pkl already exists.\n",
      "Loading...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "mnist = MnistData() # creating an object of the MnistData class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load() # loading the mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape) # shape of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNetWithBackProp(input_size=28*28, hidden_size=100, output_size=10) # creating an object of the TwoLayerNetWithBackProp class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000 # number of iterations\n",
    "train_size = x_train.shape[0] # number of training images\n",
    "batch_size = 100 # batch size\n",
    "lr = 0.1 # learning rate\n",
    "\n",
    "iter_per_ecoph = max(train_size/batch_size, 1) # iterations per epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test_acc : 0.13263333333333333, 0.1423\n",
      "train acc, test_acc : 0.9073666666666667, 0.9096\n",
      "train acc, test_acc : 0.9246, 0.927\n",
      "train acc, test_acc : 0.9396, 0.9392\n",
      "train acc, test_acc : 0.95015, 0.9482\n",
      "train acc, test_acc : 0.9558333333333333, 0.953\n",
      "train acc, test_acc : 0.9606166666666667, 0.9555\n",
      "train acc, test_acc : 0.9656666666666667, 0.9602\n",
      "train acc, test_acc : 0.96795, 0.9637\n",
      "train acc, test_acc : 0.97115, 0.9671\n",
      "train acc, test_acc : 0.97375, 0.9672\n",
      "train acc, test_acc : 0.9759333333333333, 0.9706\n",
      "train acc, test_acc : 0.97755, 0.9707\n",
      "train acc, test_acc : 0.9796333333333334, 0.9737\n",
      "train acc, test_acc : 0.97855, 0.9721\n",
      "train acc, test_acc : 0.9820333333333333, 0.973\n",
      "train acc, test_acc : 0.9832166666666666, 0.9744\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations): # training the network\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, y_batch)\n",
    "\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        network.params[key] -= lr*grads[key]\n",
    "\n",
    "    ## this is for plotting losses over time\n",
    "    train_losses.append(network.loss(x_batch, y_batch))\n",
    "\n",
    "    if i%iter_per_ecoph == 0:\n",
    "        train_acc = network.accuracy(x_train, y_train)\n",
    "        train_accs.append(train_acc)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'train acc, test_acc : {train_acc}, {test_acc}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_accs))\n",
    "plt.plot(x, train_accs, label='train acc')\n",
    "plt.plot(x, test_accs, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "my_weight_pkl_file = 'kolluru_mnist_model.pkl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle: kolluru_mnist_model.pkl is being created.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(f'{my_weight_pkl_file}', 'wb') as f: # saving the weights of the network\n",
    "    print(f'Pickle: {my_weight_pkl_file} is being created.')\n",
    "    pickle.dump(network.params, f)\n",
    "    print('Done.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.params = None # removing the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your own TwoLayerNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNetWithBackProp(input_size=28*28, hidden_size=100, output_size=10) # creating an object of the TwoLayerNetWithBackProp class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{my_weight_pkl_file}', 'rb') as f:\n",
    "    network.params = pickle.load(f)\n",
    "\n",
    "network.update_layers() # updating the layers of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_data import MnistData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle: dataset/mnist.pkl already exists.\n",
      "Loading...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "mnist = MnistData()\n",
    "(_, _), (x_test, y_test) = mnist.load() # loading the mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = network.predict(x_test[0:100]) # prediction of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1705936fa00>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQXklEQVR4nO3df2zU933H8ZcNZgk42A5gX2UTYAwa0x+zm52VzWUiHQI8bTVoYgLW4iroYAqIkLEKj3WypmmZ0UatdMtQczHDUbEImuXZTedyYLbEIyk5kvOvYjCeTLCL7+yEhUB+LNh890dUNxm+z5n7TT7Ph4Rk39vf+75y8MrXvs+dPxmSHAH43MtMdQAAyUHZAUtQdsASlB2wBGUHLDEzmSd7d/S6Qm+NJfOUgFUKFi1Qbn7OlLOYyr527Vo988wzmjFjhp5//nkdOHDA+PWht8a0s6w6llMCMHj29dqws6i/jc/MzNSzzz6riooKrVixQps3b1ZxcXG0dwcgwaIue1lZmQYGBjQ4OKhbt27p2LFjqqysjGc2AHEUddkLCws1NDQ0+fnw8LAKCwvv+DqPxyO/3y+/36+cBXOjPR2AGEVd9oyMjDtuc5w7X3nr9Xrldrvldrt1fey9aE8HIEZRl314eFgLFy6c/LyoqEhXr16NSygA8Rd12f1+v5YtW6bFixcrKytLmzZtUmtrazyzAYijqJfeJiYmtGvXLp04cUIzZszQ4cOHdf78+XhmAxBHMa2zt7W1qa2tLV5ZACQQL5cFLEHZAUtQdsASlB2wBGUHLEHZAUtQdsASlB2wBGUHLEHZAUtQdsASlB2wBGUHLEHZAUtQdsASlB2wBGUHLEHZAUtQdsASlB2wBGUHLJHULZuRGB/9YVnY2f1tbxqPdX5rhXE++M05xvnKb/QY5x2nv2Kcm3zhtQnj/L4fvx71fduIKztgCcoOWIKyA5ag7IAlKDtgCcoOWIKyA5ZgnT0NzJg/zzifePF+4/zYsu+HnYUmsozH5mT+p3H+0MzZxnlEVa9Efejotz4wzq/+YJZxvuPpJ8PO5nlfiyrTvSymsg8ODurGjRuamJjQ+Pi43G53vHIBiLOYr+yPPfaY3nnnnXhkAZBA/MwOWCKmsjuOI5/Pp3Pnzsnj8Uz5NR6PR36/X36/XzkL5sZyOgAxiOnb+PLyco2MjGjBggU6efKkLly4oI6Ojs98jdfrldfrlSRd9A/EcjoAMYjpyj4yMiJJGhsbU3Nzs8rKwr/7CkBqRV322bNnKzs7e/LjNWvWqLe3N27BAMRX1N/GFxQUqLm5+ZM7mTlTjY2NOnHiRNyC2aT/mYeM84sP10e4h/Br4fkzzEf+87vLjfM3b5izDb+faz6BwYyM28b5T774Y+M80n/bi9/7+7CzP+3bZTw28786zXd+D4q67IODgyopKYljFACJxNIbYAnKDliCsgOWoOyAJSg7YAne4poEzm//pnH+4u/8MMI9mP+afvph+KW32u9WGY994Odvm089ds04zvyfIfPxBk6mee1s+cEnjPPzf/yPxvnSrOywsw+/957x2JzvFBjn48GQcZ6OuLIDlqDsgCUoO2AJyg5YgrIDlqDsgCUoO2AJ1tmT4FaO+Vcel8wy/zXclmOcf/dfHg87W9j8qvFY86bICXbbfPbfeOpnxnnxLPPbVLsrnwk7e/kr/2o8tny1eY0/50esswNIU5QdsARlByxB2QFLUHbAEpQdsARlByzBOnsSTNyXEdPxX331O8b5Q39rXkv/vFq286xx/tLqL4Sdbcw2b0b67jffN85zfmQcpyWu7IAlKDtgCcoOWIKyA5ag7IAlKDtgCcoOWIJ19iT44l/8PKbjZ7zxQJyS2OUv/evDzjY+Zt4Ge+eXXjHOX1JeNJFSKuKVvb6+XqFQSD09PZO35eXlyefzqb+/Xz6fT7m5uYnMCCAOIpb9yJEjWrdu3Wduq66uVnt7u5YvX6729nZVV1cnLCCA+IhY9o6ODl279tktgCorK9XQ0CBJamho0Pr16xMSDkD8RPUze0FBgYLBoCQpGAwqPz8/7Nd6PB5t375dkpSzYG40pwMQBwl/Nt7r9crtdsvtduv6mHkzPQCJE1XZQ6GQXC6XJMnlcml0dDSuoQDEX1Rlb21tVVXVJ1sBV1VVqaWlJa6hAMRfxJ/ZGxsbtWrVKs2fP19DQ0OqqalRbW2tjh8/rm3btunKlSvauHFjMrKmrcyvPmycr8o9aZz33/rIOJ/ffeuuM0HKe/m+8MPHkpcjXUQs+5YtW6a8ffXq1XEPAyBxeLksYAnKDliCsgOWoOyAJSg7YAne4hoHl6pyjfNN2WPG+de7v22cz/13/91GAu7AlR2wBGUHLEHZAUtQdsASlB2wBGUHLEHZAUuwzh4HT1X8xDiP9BbWWc/Oi3CG/77LRMCduLIDlqDsgCUoO2AJyg5YgrIDlqDsgCUoO2AJ1tmT4Ifv/K5xft9LrycpCWzGlR2wBGUHLEHZAUtQdsASlB2wBGUHLEHZAUuwzj5NM3Jzws4eyBxOYhIgOhGv7PX19QqFQurp6Zm8raamRsPDwwoEAgoEAqqoqEhoSACxi1j2I0eOaN26dXfcXldXp9LSUpWWlqqtrS0h4QDET8Syd3R06Nq1a8nIAiCBon6CbteuXerq6lJ9fb1yc3PDfp3H45Hf75ff71fOgrnRng5AjKIq+6FDh7R06VKVlJRoZGREBw8eDPu1Xq9Xbrdbbrdb18feizoogNhEVfbR0VHdvn1bjuPI6/WqrKws3rkAxFlUZXe5XJMfb9iwQb29vXELBCAxIq6zNzY2atWqVZo/f76GhoZUU1OjVatWqaSkRI7j6PLly9qxY0cysqbU8LYvhZ39yQP/YTz2zfcXxzkNpuN/f/961Md+cHtWHJOkh4hl37Jlyx23HT58OCFhACQOL5cFLEHZAUtQdsASlB2wBGUHLMFbXHHPGv/GI8b5sdJ/Mkx/zXhs84HfM85z9DPjPB1xZQcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKssyNtRVpHv/bk+8b5w1nh19Kf+EW58djcF980zh3jND1xZQcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKss0/T3MsTYWeXxz9IYpLPj4yZ5n9+7z51wzg/97VjxvnJD+8PO+v/q/C/GlySZt06Z5zfi7iyA5ag7IAlKDtgCcoOWIKyA5ag7IAlKDtgCdbZp2lO09mws5/+TbHx2KX3jRnnl4q+bJyPD//COE+l218vMc4Hnwg/+6PiTuOxT+eb19EjefrPq8LO7j/xekz3fS+KeGUvKirS6dOndf78efX29mr37t2SpLy8PPl8PvX398vn8yk3NzfRWQHEIGLZx8fHtXfvXq1YsUKPPvqodu7cqeLiYlVXV6u9vV3Lly9Xe3u7qqurk5EXQJQilj0YDCoQCEiSbt68qb6+PhUWFqqyslINDQ2SpIaGBq1fvz6hQQHE5q5+Zl+0aJFKS0t19uxZFRQUKBgMSvrkfwj5+flTHuPxeLR9+3ZJUs6CuTHGBRCtaT8bP2fOHDU1NWnPnj26ccP8BoVP83q9crvdcrvduj72XlQhAcRuWmWfOXOmmpqadPToUTU3N0uSQqGQXC6XJMnlcml0dDRxKQHEbFrfxtfX16uvr091dXWTt7W2tqqqqkoHDhxQVVWVWlpaEhbyXvdE7qBxHnrJ/OPNuWsPxTNOXNUuec44L5kV/eruGx+Hf1uxJH379W3G+dLTF8LOzPf8+RTxb6K8vFxbt25Vd3f35BN1+/fvV21trY4fP65t27bpypUr2rhxY8LDAohexLKfOXNGGRkZU85Wr14d90AAEoOXywKWoOyAJSg7YAnKDliCsgOW4C2ucXDkH/7AOB998hXj/K8XdJlPEGmeUuZ/QuOGFe2uj833/K0XdxvnS6pfM85tXEs34coOWIKyA5ag7IAlKDtgCcoOWIKyA5ag7IAlWGePgwcPm9d7/a8sN86//28fGed/lnfprjMly8MvP26cz+qZHXZW9HevGo9dIvPjirvDlR2wBGUHLEHZAUtQdsASlB2wBGUHLEHZAUuwzp4EEwPm3xt/6ssPmOf6WjzjxNWvqzPVETBNXNkBS1B2wBKUHbAEZQcsQdkBS1B2wBKUHbBExLIXFRXp9OnTOn/+vHp7e7V79ye/y7umpkbDw8MKBAIKBAKqqKhIeFgA0Yv4oprx8XHt3btXgUBA2dnZeuONN3Ty5ElJUl1dnQ4ePJjwkABiF7HswWBQwWBQknTz5k319fWpsLAw4cEAxNdd/cy+aNEilZaW6uzZs5KkXbt2qaurS/X19crNzZ3yGI/HI7/fL7/fr5wFc2MODCA60y77nDlz1NTUpD179ujGjRs6dOiQli5dqpKSEo2MjIT9dt7r9crtdsvtduv62HtxCw7g7kyr7DNnzlRTU5OOHj2q5uZmSdLo6Khu374tx3Hk9XpVVlaW0KAAYjOtstfX16uvr091dXWTt7lcrsmPN2zYoN7e3vinAxA3EZ+gKy8v19atW9Xd3a1AICBJ2r9/vzZv3qySkhI5jqPLly9rx44dCQ8LIHoRy37mzBllZGTccXtbW1tCAgFIDF5BB1iCsgOWoOyAJSg7YAnKDliCsgOWoOyAJSg7YAnKDliCsgOWoOyAJSg7YAnKDliCsgOWyJDkJOtko6OjeuuttyY/nz9/vt5+++1knf6upGu2dM0lkS1a8cy2aNEi5efnh507qfrj9/tTdu57NVu65iJb+mfj23jAEpQdsERKy/7cc8+l8vRG6ZotXXNJZItWsrIl9Qk6AKnDt/GAJSg7YImUlH3t2rW6cOGCLl26pH379qUiQliDg4OTvyPf7/enNEt9fb1CoZB6enomb8vLy5PP51N/f798Pl/YPfZSkS1dtvEOt814qh+7dNj+PKlripmZmc7AwICzZMkSJysry+ns7HSKi4tTvtb5yz+Dg4POvHnzUp5DkrNy5UqntLTU6enpmbztwIEDzr59+xxJzr59+5za2tq0yVZTU+Ps3bs35Y+by+VySktLHUlOdna2c/HiRae4uDjlj124XMl63JJ+ZS8rK9PAwIAGBwd169YtHTt2TJWVlcmOcU/o6OjQtWvXPnNbZWWlGhoaJEkNDQ1av359CpJNnS1dBIPByd2LPr3NeKofu3C5kiXpZS8sLNTQ0NDk58PDw2m137vjOPL5fDp37pw8Hk+q49yhoKBAwWBQ0if/eEwvjUyF6WzjnUyf3mY8nR67aLY/j1XSyz7VVlKO4yQ7Rljl5eV65JFHVFFRoZ07d2rlypWpjnTPmO423sny/7cZTxfRbn8eq6SXfXh4WAsXLpz8vKioSFevXk12jLBGRkYkSWNjY2pubk67rahDodDkDroul0ujo6MpTvQr6bSN91TbjKfDY5fK7c+TXna/369ly5Zp8eLFysrK0qZNm9Ta2prsGFOaPXu2srOzJz9es2ZN2m1F3draqqqqKklSVVWVWlpaUpzoV9JpG++pthlPh8cu1dufJ/3Z0oqKCufixYvOwMCAs3///pQ/e/vLP0uWLHE6Ozudzs5Op7e3N+XZGhsbnatXrzoff/yxMzQ05Dz++OPOgw8+6Jw6dcrp7+93Tp065eTl5aVNthdeeMHp7u52urq6nJaWFsflcqUkW3l5ueM4jtPV1eUEAgEnEAg4FRUVKX/swuVK1uPGy2UBS/AKOsASlB2wBGUHLEHZAUtQdsASlB2wBGUHLPF/y1z6ZQ3mmR8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[10].reshape(28,28)) # displaying the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.04469713, -3.49668219,  5.01952293, -0.91343689, -6.93159854,\n",
       "       -0.37976285, -1.07778246, -2.12899779, -4.740167  ,  2.28737169])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.04469713, -3.49668219,  5.01952293, -0.91343689, -6.93159854,\n",
       "       -0.37976285, -1.07778246, -2.12899779, -4.740167  ,  2.28737169])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_hat[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Handwritten images to test twolayernetwithbackprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 0_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 0_0.png is for digit 0 and recognized as 0.\n",
      "Testing 0_1.png...\n",
      "Figure(640x480)\n",
      "Success: Image 0_1.png is for digit 0 and recognized as 0.\n",
      "Testing 0_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 0_2.png is for digit 0 and recognized as 0.\n",
      "Testing 0_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 0_3.png is for digit 0 and recognized as 0.\n",
      "Testing 0_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 0_4.png is for digit 0 and recognized as 0.\n",
      "Testing 1_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 1_0.png is for digit 1 and recognized as 1.\n",
      "Testing 1_1.png...\n",
      "Figure(640x480)\n",
      "Success: Image 1_1.png is for digit 1 and recognized as 1.\n",
      "Testing 1_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 1_2.png is for digit 1 and recognized as 1.\n",
      "Testing 1_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 1_3.png is for digit 1 and recognized as 1.\n",
      "Testing 1_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 1_4.png is for digit 1 and recognized as 1.\n",
      "Testing 2_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 2_0.png is for digit 2 and recognized as 2.\n",
      "Testing 2_1.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 2_1.png is for digit 2 but the inference result is 8.\n",
      "Testing 2_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 2_2.png is for digit 2 and recognized as 2.\n",
      "Testing 2_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 2_3.png is for digit 2 and recognized as 2.\n",
      "Testing 2_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 2_4.png is for digit 2 and recognized as 2.\n",
      "Testing 3_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 3_0.png is for digit 3 and recognized as 3.\n",
      "Testing 3_1.png...\n",
      "Figure(640x480)\n",
      "Success: Image 3_1.png is for digit 3 and recognized as 3.\n",
      "Testing 3_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 3_2.png is for digit 3 and recognized as 3.\n",
      "Testing 3_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 3_3.png is for digit 3 and recognized as 3.\n",
      "Testing 3_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 3_4.png is for digit 3 and recognized as 3.\n",
      "Testing 4_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 4_0.png is for digit 4 and recognized as 4.\n",
      "Testing 4_1.png...\n",
      "Figure(640x480)\n",
      "Success: Image 4_1.png is for digit 4 and recognized as 4.\n",
      "Testing 4_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 4_2.png is for digit 4 and recognized as 4.\n",
      "Testing 4_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 4_3.png is for digit 4 and recognized as 4.\n",
      "Testing 4_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 4_4.png is for digit 4 and recognized as 4.\n",
      "Testing 5_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 5_0.png is for digit 5 and recognized as 5.\n",
      "Testing 5_1.png...\n",
      "Figure(640x480)\n",
      "Success: Image 5_1.png is for digit 5 and recognized as 5.\n",
      "Testing 5_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 5_2.png is for digit 5 and recognized as 5.\n",
      "Testing 5_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 5_3.png is for digit 5 and recognized as 5.\n",
      "Testing 5_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 5_4.png is for digit 5 and recognized as 5.\n",
      "Testing 6_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 6_0.png is for digit 6 and recognized as 6.\n",
      "Testing 6_1.png...\n",
      "Figure(640x480)\n",
      "Success: Image 6_1.png is for digit 6 and recognized as 6.\n",
      "Testing 6_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 6_2.png is for digit 6 and recognized as 6.\n",
      "Testing 6_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 6_3.png is for digit 6 and recognized as 6.\n",
      "Testing 6_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 6_4.png is for digit 6 and recognized as 6.\n",
      "Testing 7_0.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 7_0.png is for digit 7 but the inference result is 3.\n",
      "Testing 7_1.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 7_1.png is for digit 7 but the inference result is 3.\n",
      "Testing 7_2.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 7_2.png is for digit 7 but the inference result is 3.\n",
      "Testing 7_3.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 7_3.png is for digit 7 but the inference result is 9.\n",
      "Testing 7_4.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 7_4.png is for digit 7 but the inference result is 9.\n",
      "Testing 8_0.png...\n",
      "Figure(640x480)\n",
      "Success: Image 8_0.png is for digit 8 and recognized as 8.\n",
      "Testing 8_1.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 8_1.png is for digit 8 but the inference result is 3.\n",
      "Testing 8_2.png...\n",
      "Figure(640x480)\n",
      "Success: Image 8_2.png is for digit 8 and recognized as 8.\n",
      "Testing 8_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 8_3.png is for digit 8 and recognized as 8.\n",
      "Testing 8_4.png...\n",
      "Figure(640x480)\n",
      "Success: Image 8_4.png is for digit 8 and recognized as 8.\n",
      "Testing 9_0.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 9_0.png is for digit 9 but the inference result is 3.\n",
      "Testing 9_1.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 9_1.png is for digit 9 but the inference result is 3.\n",
      "Testing 9_2.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 9_2.png is for digit 9 but the inference result is 7.\n",
      "Testing 9_3.png...\n",
      "Figure(640x480)\n",
      "Success: Image 9_3.png is for digit 9 and recognized as 9.\n",
      "Testing 9_4.png...\n",
      "Figure(640x480)\n",
      "Fail: Image 9_4.png is for digit 9 but the inference result is 8.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory with images \n",
    "image_dir = \"./\"\n",
    "\n",
    "# Loop to test each handwritten digit image\n",
    "for digit in range(10):\n",
    "    for index in range(5):  # since there are five images per digit\n",
    "        filename = f\"{digit}_{index}.png\"\n",
    "        filepath = os.path.join(image_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Testing {filename}...\")\n",
    "            # Running the Python script with image filename and correct digit as arguments\n",
    "            !python module6.py {filename} {digit}\n",
    "        else:\n",
    "            print(f\"{filename} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
